{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Given:</b><br><br>\n",
    "A small dataset with flat sizes (x) and flat prices (y)<br>\n",
    "Two data points for simplicity - a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |\n",
    "\n",
    "<b>Find:</b><br><br>\n",
    "Linear regression function by minimizing squared error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1,2])\n",
    "y_train = np.array([300,500])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Function $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateLoss(x,y,w,b):\n",
    "    \n",
    "    f_wb = y - (w*x + b)\n",
    "    m = np.shape(x)[0]\n",
    "    cost = np.sum(f_wb**2) / (2*m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Gradient  -  $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradient(x,y,w,b):\n",
    "    dJ_dw = np.sum(((w*x + b) - y)*x)\n",
    "    dJ_db = np.sum(((w*x + b) - y))\n",
    "    return dJ_dw, dJ_db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Process<br><br>\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(x,y,w_init,b_init):\n",
    "\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "\n",
    "    alpha = 0.01 # learning rate\n",
    "    iterations = 10000\n",
    "\n",
    "    log = [[w,b,CalculateLoss(x,y,w,b)]]\n",
    "\n",
    "    i = 0\n",
    "    while i < iterations:\n",
    "\n",
    "        dJ_dw, dJ_db = ComputeGradient(x,y,w,b)\n",
    "        w = w - alpha*dJ_dw\n",
    "        b = b - alpha*dJ_db\n",
    "\n",
    "        i = i+1\n",
    "        log.append([w,b,CalculateLoss(x,y,w,b)])\n",
    "        #print(w, b, CalculateLoss(x,y,w,b))\n",
    "        \n",
    "    log = np.array(log)\n",
    "\n",
    "    return w, b, log\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random w & b points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rand = np.random.uniform(low=-10, high=10)\n",
    "b_rand = np.random.uniform(low=-10, high=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, log = GD(x_train,y_train,w_rand,b_rand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w,b,Err2(w,b) surface to visualize the descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 = []\n",
    "\n",
    "for w_ in range (0, 300):\n",
    "    for b_ in range (0, 200):\n",
    "        error = CalculateLoss(x_train, y_train, w_, b_)\n",
    "        err2.append([w_,b_,error])\n",
    "\n",
    "err2 = np.array(err2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize surface and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x128cf9430>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"Loss\")\n",
    "\n",
    "from matplotlib import cm\n",
    "w_b_surface = ax.plot_trisurf(err2[:,0], err2[:,1], err2[:,2], cmap=cm.jet, linewidth=0)\n",
    "fig.colorbar(w_b_surface, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.scatter(log[:,0],log[:,1],log[:,2], c='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
