{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gradient for Linear Regression with Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for the cost function regularized linear regression is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ \n",
    "\n",
    "Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "\n",
    "The difference is the regularization term,  <span style=\"color:red\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunction_Linear(x,y,w,b,lambda_):\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    loss = np.dot(x,w) + b - y\n",
    "    loss2 = np.sum(loss**2) / 2 / m\n",
    "    regularization = lambda_ * np.sum(w**2) / 2 / m\n",
    "\n",
    "    cost = loss2 + regularization\n",
    "\n",
    "    return loss, cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gradient for Logistic Regression with Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularized logistic regression, the cost function is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$ \n",
    "\n",
    "Compare this to the cost function without regularization (which you implemented in  a previous lab):\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n",
    "The difference is the regularization term,  <span style=\"color:red\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunction_Logistics(x,y,w,b,lambda_):\n",
    "\n",
    "    def gfx(x,w,b):\n",
    "        return 1 / (1+np.exp(-(np.dot(x,w)+b)))\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    loss = gfx(x,w,b) - y\n",
    "\n",
    "    cost_first_log = np.log(gfx(x,w,b))\n",
    "    \n",
    "    cost_second_log = np.log(1 - gfx(x,w,b))\n",
    "\n",
    "    cost_regularization = lambda_ * np.sum(w**2) / 2\n",
    "\n",
    "    cost_sum = -1 * (np.dot(y,cost_first_log) + np.dot((1-y),cost_second_log)) + cost_regularization\n",
    "\n",
    "    cost = cost_sum / m\n",
    "\n",
    "    return loss, cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gradient (same for both regressions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradient(x,y,w,b,lambda_,loss=\"linear\"):\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    if loss == \"linear\":\n",
    "        loss_function = CostFunction_Linear\n",
    "    else:\n",
    "        loss_function = CostFunction_Logistics\n",
    "\n",
    "    loss, _ = loss_function(x,y,w,b,lambda_)\n",
    "\n",
    "    dJ_dw = np.dot(loss,x) / m + lambda_ * w / m\n",
    "    dJ_db = np.sum(loss) / m\n",
    "\n",
    "    return dJ_dw, dJ_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: [0.17380013 0.32007508 0.10776313]\n",
      "Regularized dj_dw:\n",
      " 0.341798994972791\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  ComputeGradient(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp, \"\")\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
