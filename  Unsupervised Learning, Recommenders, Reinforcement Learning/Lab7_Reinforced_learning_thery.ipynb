{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept 1 - Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "| - | ------ | ------ | ------ | ------ | ------ | ------ |\n",
    "| Return | 100     | 50 | 25 | 12.5 | 20 | 40 |\n",
    "| Policy $\\pi$ | | ← | ← | ← | → | |\n",
    "| Reward | 100    | 0      | 0      | 0      | 0      | 40     |\n",
    "\n",
    "* $\\pi$ is a policy, can be one of the actions ← or →\n",
    "* inflation $\\gamma=0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next step is 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy(current_state, obey_rate):\n",
    "\n",
    "    optimal_way = {1 : ['left', 0], 2:['left', 1], 3:['left', 2], 4:['right', 5]} # as in table above\n",
    "    not_optimal_way = {1 : ['right', 2], 2:['right', 3], 3:['right', 4], 4:['left', 3]}\n",
    "\n",
    "    rng = np.random.rand()\n",
    "    next_step = optimal_way[current_state][1] if rng < obey_rate else not_optimal_way[current_state][1]\n",
    "\n",
    "    return next_step\n",
    "\n",
    "next_step = policy(3,0.9)\n",
    "print(f\"next step is {next_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept 2 - MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP - Markov Decision Process\n",
    "The Future depends on where you are Now, not how you got here. Discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept 3 - State value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a)$ = Reward, where\n",
    "* start in a state $s$, \n",
    "* take action $a$,\n",
    "* behave optimally after that\n",
    "\n",
    "Example:\n",
    "\n",
    "$Q(2,→) = 0 + \\gamma^1 \\cdot Q(3,←) = 0 + \\gamma^1 \\cdot 0 + \\gamma^2 \\cdot Q(2,←) = 0 + \\gamma^2 \\cdot 0 + \\gamma^3 \\cdot Q(1,←) = 0 + 0.125 \\cdot 100 = 12.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman's formula:\n",
    "\n",
    "$Q(s,a) = R(s) + \\gamma \\cdot \\underset{all \\space new \\space a^{,}} \\max Q(s^{,},a^{,})$, where\n",
    "* $R(s)$ - reward at current state $s$\n",
    "* $a$ - current action\n",
    "* $s^{,}$ - new state\n",
    "* $a^{,}$ - next action\n",
    "\n",
    "Example:\n",
    "\n",
    "| State | 1 | 2 | 3 | 4 | 5 | 6 |\n",
    "| - | ------ | ------ | ------ | ------ | ------ | ------ |\n",
    "| Return | 100     | ←50 - 12.5→| ←25 - 6.25→ | ←12.5 - 10→ | ←6.25 - 20→ | 40 |\n",
    "\n",
    "\n",
    "\n",
    "$Q(2,→) = R(2) + \\gamma \\cdot \\underset{all \\space new \\space a^{,}} \\max Q(3,a^{,}) = 0 + 0.5 \\cdot \\underset{all \\space new \\space a^{,}} \\max (25, 6.25)) = 0.5*25 = 12.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept 4 - Stochastic Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s, a)$ will have 90% chance to obey and go direction $a$, and 10% to disobeay and go other direction\n",
    "\n",
    "Goal - choose policy $\\pi$ that maximise Expected Return (over a batch of 1000 applied policies):\n",
    "$G = E[R_1 + \\gamma \\cdot R_2 + \\gamma \\cdot R_3 + \\gamma \\cdot R_4 + ...]$\n",
    "\n",
    "Bellman's function $Q(s,a) = R(s) + \\gamma \\cdot \\underset{all \\space new \\space a^{,}} E[\\max Q(s^{,},a^{,})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(1) = 45.41, Q(2) = 21.33, Q(3) = 10.34, Q(4) = 18.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bellman_function(current_state, obey_rate = 1):\n",
    "\n",
    "    states = np.array([1,2,3,4,5,6])\n",
    "    actions = np.array([-1, 1]) # left / right\n",
    "    rewards = np.array([100,0,0,0,0,40])\n",
    "    inflation_rate = 0.5\n",
    "    obey_rate = 0.9\n",
    "\n",
    "    optimal_way = {1 : ['left', 0], 2:['left', 1], 3:['left', 2], 4:['right', 5]} # as in table above\n",
    "    not_optimal_way = {1 : ['right', 2], 2:['right', 3], 3:['right', 4], 4:['left', 3]}\n",
    "\n",
    "    stochastic_rewards = []\n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "        intermediate_state = current_state\n",
    "        intermediate_reward = rewards[current_state] # Q(s,a) = R(s) + ...\n",
    "        intermediate_step = 0\n",
    "\n",
    "        while intermediate_reward==0:\n",
    "\n",
    "            rng = np.random.rand()\n",
    "\n",
    "            intermediate_step += 1\n",
    "            intermediate_state = policy(intermediate_state, obey_rate)\n",
    "            intermediate_reward += rewards[intermediate_state] * inflation_rate**intermediate_step # Q(s,a) = ... + y * Q(s',a')\n",
    "\n",
    "        stochastic_rewards.append(intermediate_reward)\n",
    "\n",
    "    result = np.mean(stochastic_rewards)\n",
    "\n",
    "    return result\n",
    "\n",
    "stochasticQ1 = bellman_function(1, obey_rate = 0.9)\n",
    "stochasticQ2 = bellman_function(2, obey_rate = 0.9)\n",
    "stochasticQ3 = bellman_function(3, obey_rate = 0.9)\n",
    "stochasticQ4 = bellman_function(4, obey_rate = 0.9)\n",
    "\n",
    "print(f\"Q(1) = {stochasticQ1:.2f}, Q(2) = {stochasticQ2:.2f}, Q(3) = {stochasticQ3:.2f}, Q(4) = {stochasticQ4:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforceent Learning\n",
    "\n",
    "Teach NN to calculate best reward action $a$.\n",
    "\n",
    "Compute policy $\\pi$, so \n",
    "$\n",
    "\\pi : f\n",
    "\\begin{bmatrix} \n",
    "\\vec{x} \\\\\n",
    "a\n",
    "\\end{bmatrix} = \n",
    "Q(s,a)\n",
    "$, where\n",
    "$\\vec{x} = \\begin{bmatrix} oX \\\\ oY \\\\ oZ \\\\ oX^{'} \\\\ oY^{'} \\\\ oZ^{'} \\end{bmatrix}$,  and\n",
    "$a = \\begin{bmatrix} 0_{left} \\\\ 0_{right} \\\\ 1_{up} \\\\ 0_{down} \\end{bmatrix}$\n",
    "\n",
    "For each action $a$ calculate the policy $\\pi$, and pick max value \n",
    "* Q(s, nothing), \n",
    "* Q(s, left), \n",
    "* Q(s, right),\n",
    "* etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset by randomly picking actions and recording outcomes: $(s, a, R(s), s^{'})$. Now, we have 10'000 of records $(s, a, R(s), s^{'})$, create a NN to solve $Q(s,a) = R(s) + \\gamma \\cdot \\underset{all \\space new \\space a^{,}} \\max Q(s^{,},a^{,})$:\n",
    "* inputs x = $(s,a)$, and \n",
    "* targets y = $R(s) + \\gamma \\cdot \\underset{all \\space new \\space a^{,}} \\max Q(s^{,},a^{,})$\n",
    "    * Deep Q network algorithm:\n",
    "    * initially, $Q(s^{,},a^{,})$ will become a random guess (much like in GD)\n",
    "    * create new NN \"DQN\" so it can approximate $Q_{new}(s,a)$ = y targets\n",
    "* update Q to $Q_{new}$\n",
    "* repeat algorithm and gradually improve Q function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
